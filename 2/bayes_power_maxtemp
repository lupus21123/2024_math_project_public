import os
import sys
import argparse
import logging
from collections import defaultdict
import math

def training(instances, labels):
    separated = defaultdict(list)
    for instance, label in zip(instances, labels):
        separated[label].append(instance)

    summaries = {}
    for class_value, rows in separated.items():
        summaries[class_value] = [
            (mean(column), variance(column))
            for column in zip(*rows)
        ]
    return summaries

def predict(instance, parameters):
    probabilities = {}
    for class_value, class_summaries in parameters.items():
        probabilities[class_value] = calculate_class_probability(instance, class_summaries)
    return max(probabilities, key=probabilities.get)

def calculate_class_probability(instance, summaries):
    probability = 1
    for i in range(len(instance)):
        mean, var = summaries[i]
        probability *= gaussian_probability(instance[i], mean, var)
    return probability

def gaussian_probability(x, mean, variance):
    if variance == 0:
        return 1 if x == mean else 0
    exponent = math.exp(-(math.pow(x - mean, 2) / (2 * variance)))
    return (1 / math.sqrt(2 * math.pi * variance)) * exponent

def mean(numbers):
    return sum(numbers) / len(numbers)

def variance(numbers):
    avg = mean(numbers)
    return sum((x - avg) ** 2 for x in numbers) / len(numbers)

def load_raw_data(fname):
    instances = []
    labels = []
    with open(fname, "r") as f:
        f.readline()
        for line in f:
            tmp = line.strip().split(", ")
            features = [float(tmp[2]), float(tmp[7])]
            label = int(tmp[8])
            instances.append(features)
            labels.append(label)
    return instances, labels

def report(predictions, answers):
    if len(predictions) != len(answers):
        logging.error("The lengths of two arguments should be same")
        sys.exit(1)

    correct = 0
    for idx in range(len(predictions)):
        if predictions[idx] == answers[idx]:
            correct += 1
    accuracy = round(correct / len(answers), 2) * 100

    tp = 0
    fp = 0
    for idx in range(len(predictions)):
        if predictions[idx] == 1:
            if answers[idx] == 1:
                tp += 1
            else:
                fp += 1
    precision = round(tp / (tp + fp), 2) * 100

    tp = 0
    fn = 0
    for idx in range(len(answers)):
        if answers[idx] == 1:
            if predictions[idx] == 1:
                tp += 1
            else:
                fn += 1
    recall = round(tp / (tp + fn), 2) * 100

    f1_score = round((2 * precision * recall) / (precision + recall), 2) if (precision + recall) > 0 else 0

    logging.info("accuracy: {}%".format(accuracy))
    logging.info("precision: {}%".format(precision))
    logging.info("recall: {}%".format(recall))
    logging.info("f1_score: {}%".format(f1_score))

def run(train_file, test_file):
    instances, labels = load_raw_data(train_file)
    logging.debug("instances: {}".format(instances))
    logging.debug("labels: {}".format(labels))
    parameters = training(instances, labels)

    instances, labels = load_raw_data(test_file)
    predictions = []
    for instance in instances:
        result = predict(instance, parameters)

        if result not in [0, 1]:
            logging.error("The result must be either 0 or 1")
            sys.exit(1)

        predictions.append(result)
    
    report(predictions, labels)

def command_line_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-t", "--training", required=True, metavar="<file path to the training dataset>", help="File path of the training dataset", default="training.csv")
    parser.add_argument("-u", "--testing", required=True, metavar="<file path to the testing dataset>", help="File path of the testing dataset", default="testing.csv")
    parser.add_argument("-l", "--log", help="Log level (DEBUG/INFO/WARNING/ERROR/CRITICAL)", type=str, default="INFO")
    args = parser.parse_args()
    return args

def main():
    args = command_line_args()
    logging.basicConfig(level=args.log)

    if not os.path.exists(args.training):
        logging.error("The training dataset does not exist: {}".format(args.training))
        sys.exit(1)

    if not os.path.exists(args.testing):
        logging.error("The testing dataset does not exist: {}".format(args.testing))
        sys.exit(1)

    run(args.training, args.testing)

if __name__ == "__main__":
    main()
